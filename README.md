# ì–¸ì–´ ê¸°ë°˜ ì œë¡œ-ìƒ· ë¬¼ì²´ ëª©í‘œ íƒìƒ‰ ì´ë™ ì‘ì—…ë“¤ì„ ìœ„í•œ ì¸ê³µì§€ëŠ¥ ê¸°ì € ëª¨ë¸ë“¤ì˜ í™œìš©
**Utilizing AI Foundation Models for Language Driven Zero-Shot Object Navigation Tasks**
**(Journal of Korea Robotics Society, 2024)**
###### [[Paper]](https://jkros.org/xml/41654/41654.pdf)


<div style="margin:50px; text-align: justify;">
<img style="width:70%;" src="media/LZSON_Main_GIF.gif">

ë³¸ ì—°êµ¬ëŠ” ì–¸ì–´ ê¸°ë°˜ ì œë¡œ-ìƒ· ë¬¼ì²´ ëª©í‘œ íƒìƒ‰ ì´ë™(Language-Driven Zero-Shot Object Navigation, L-ZSON) ì‘ì—…ì„ ìœ„í•œ ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ì œì•ˆí•©ë‹ˆë‹¤. <p>
L-ZSONì€ ë¡œë´‡ ì—ì´ì „íŠ¸ê°€ ì´ì „ì— ê²½í—˜í•˜ì§€ ì•Šì€ ë¯¸ì§€ì˜ í™˜ê²½ì—ì„œ ìì—°ì–´ ë¬˜ì‚¬ì™€ ì‹¤ì‹œê°„ RGB-D ì…ë ¥ ì˜ìƒë§Œì„ í™œìš©í•˜ì—¬ ëª©í‘œ ë¬¼ì²´ë¥¼ íƒìƒ‰í•˜ê³  ì´ë™ ê³„íšì„ ìˆ˜ë¦½í•˜ë©° ì‹¤í–‰í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
</div>

<div style="margin:50px; text-align: justify;">
ì´ ëª¨ë¸ì€ ì•„ë˜ì™€ ê°™ì€ ì„¸ ê°€ì§€ ì£¼ìš” ê³¼ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤: <br>
<p> 1. ëª©í‘œ ë¬¼ì²´ ê·¸ë¼ìš´ë”©(Target Object Grounding): ìì—°ì–´ ë¬˜ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª©í‘œ ë¬¼ì²´ë¥¼ ì •í™•íˆ ì‹ë³„í•©ë‹ˆë‹¤. ì´ëŠ” ë¬¼ì²´ì˜ ì†ì„±ì´ë‚˜ ë‹¤ë¥¸ ë¬¼ì²´ì™€ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ì‹¤ì‹œê°„ RGB ì˜ìƒì—ì„œ ëª©í‘œ ë¬¼ì²´ì™€ ë°©ì„ íƒì§€í•©ë‹ˆë‹¤.</p>
<img style="width:80%;" src="media/problem1.png">
<br> <br>
<p> 2. ê³µê°„ì  ë§¥ë½ ì§€ë„ ìƒì„±(Spatial Context Map Generation): ì‹¤ì‹œê°„ RGB-D ì…ë ¥ ì˜ìƒìœ¼ë¡œë¶€í„° ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì„ ì´í•´í•˜ê³ , ë¬¼ì²´ì™€ ë°© ê°„ì˜ ê³µê°„ì  ê´€ê³„ë¥¼ ë°˜ì˜í•œ ì˜ë¯¸ì  ë§¥ë½ ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.</p>
<img style="width:80%;" src="media/problem2.png">
<br> <br>
<p> 3. íš¨ìœ¨ì ì¸ íƒí—˜ í–‰ë™(Efficient Exploratory Actions): ëª©í‘œ ë¬¼ì²´ê°€ ì‹œì•¼ì— ë“¤ì–´ì˜¤ê¸° ì „ê¹Œì§€ íš¨ìœ¨ì ì¸ íƒìƒ‰ í–‰ë™ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ íƒí—˜ ê²½ë¡œë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.</p>
<img style="width:80%;" src="media/problem3.png">
<br> <br>
</div>

This repository contains code for CoWs on Pasture.

## Dependencies

Create the conda environment:
```sh
conda env create -f environment.yml
```
Activate the environment:
```sh
conda activate lzson_agent
```

```
python scripts/test_torch_download.py
```
## Prepare
### Pasture ë°ì´í„° ì§‘í•© ë‹¤ìš´ë¡œë“œ
Pasture THOR binaries (~4GB)
```sh
wget https://cow.cs.columbia.edu/downloads/pasture_builds.tar.gz
```
```sh
tar -xvf pasture_builds.tar.gz
```

### ê° ì—í”¼ì†Œë“œ, í‰ê°€ìš© ë°ì´í„° ë‹¤ìš´ë¡œë“œ
```sh
wget https://cow.cs.columbia.edu/downloads/datasets.tar.gz
```
```sh
tar -xvf datasets.tar.gz
```

---
### **ğŸ“‚ Directory Structure**

```
src
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ agent_fbe.py
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ exploration
â”‚   â”‚   â”œâ”€â”€ frontier_based_exploration.py
â”‚   â”œâ”€â”€ prompting
â”‚   â”‚   â”œâ”€â”€ ContextMapGeneration.py
â”‚   â”‚   â”œâ”€â”€ LLM_Llama2_7b.py
â”‚   â”‚   â”œâ”€â”€ LLM_Prompting.py
â”œâ”€â”€ shared
â”‚   â”œâ”€â”€ data_split.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ simulation
    â”œâ”€â”€ constants.py
    â”œâ”€â”€ sim_enums.py
    â”œâ”€â”€ utils.py
    â””â”€â”€ visualization_utils.py
```

#### ì£¼ìš” ëª¨ë“ˆ ì„¤ëª…

- **ì—ì´ì „íŠ¸ êµ¬ì¡°**:
  - ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” `src/models/agent.py`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ëŠ” [RoboTHOR](https://github.com/allenai/robothor-challenge/blob/main/robothor_challenge/agent.py)ì—ì„œ í™•ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.

- **í”„ë¡ í‹°ì–´ ê¸°ë°˜ íƒí—˜(FBE)**:
  - `src/models/exploration/frontier_based_exploration.py`ì— êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©°, ëª©í‘œë¥¼ ì°¾ê¸° ì „ íƒí—˜ì„ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ë¡œì§ì„ í¬í•¨í•©ë‹ˆë‹¤.
  - ì´ëŠ” [CoWs_on_Pasture](https://github.com/real-stanford/cow)ì— ê³µê°„ì  ë§¥ë½ ì§€ë„ êµ¬ì„± ë°©ë²•ì„ ì‹ ê·œ ì¶”ê°€í•˜ì—¬ í™•ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.
  - `src/models/agent_fbe.py`ì—ì„œëŠ” ì´ FBE ì•Œê³ ë¦¬ì¦˜ì„ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ëª©í‘œë¥¼ ë°œê²¬í•œ í›„ ì´ë¥¼ í™œìš©í•˜ëŠ” ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
  - 
- **Context Map Generation (CMG)**:
  - `src/models/prompting/ContextMapGeneration.py`ì— êµ¬í˜„ëœ ëª¨ë“ˆë¡œ, ê·¸ë˜í”„ ë…¸ë“œ í˜•íƒœì˜ ì˜ë¯¸ì  ë¬¼ì²´ ì§€ë„ ì‘ì„±ê³¼ ë¬¼ì²´ë“¤ ê°„ì˜ ê³µê°„ ê´€ê³„ë¥¼ í†µí•´ ë§¥ë½ ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 
- **Context-based LLM Prompting (CLP)**:
  - `src/models/prompting/LLM_Prompting.py`ì— êµ¬í˜„ëœ ëª¨ë“ˆë¡œ, ë§¥ë½ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª©í‘œê°€ ìˆì„ ë²•í•œ ë°©ê³¼ ëª©í‘œ ê·¼ì²˜ì— ìˆì„ ë²•í•œ ë¬¼ì²´ë“¤ì˜ ì •ë³´ë“¤ì„ LLMì—ê²Œ ì§ˆì˜í•©ë‹ˆë‹¤.

- **ë¡œì»¬ë¼ì´ì œì´ì…˜(Localization)**:
  - `src/models/localization/` ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“ˆì€ ëª©í‘œê°€ ì‹œì•¼ì— ë“¤ì–´ì™”ì„ ë•Œ í•´ë‹¹ ëª©í‘œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ë¡œì§ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

---


## Run Code on Pasture and RoboTHOR

```
python pasture_runner.py -a src.models.agent_fbe_lzson -n 8 --arch B32 --center
```

Note: this automatically evaluates all Pasture splits and RoboTHOR. 


### Visualization on Pasture

```
python path_visualization.py --out-dir viz/ --thor-floor FloorPlan_Val3_5 --result-json media/media_data/FloorPlan_Val3_5_GingerbreadHouse_1.json --thor-build pasture_builds/thor_build_longtail/longtail.x86_64
```

The script outputs 1) egocentric pngs for each view, 2) an mp4 for the egocentric feed, 3) top-down pngs for each pose, 4) an mp4 for the top-down feed.

### Evaluation

```
python success_agg.py --result-dir results/longtail_longtail_fbe_owl-b32-openai-center/
```

---
## Result
### ì •ëŸ‰ì  í‰ê°€
- ![performance](media/performance1.png)

### ì •ì„±ì  í‰ê°€
- ![performance](media/performance2.png)
- ![performance](media/performance3.png)
---

## Paper

```bibtex
@article{ìµœì •í˜„2024ì–¸ì–´,
  title={ì–¸ì–´-ê¸°ë°˜ ì œë¡œ-ìƒ· ë¬¼ì²´ ëª©í‘œ íƒìƒ‰ ì´ë™ ì‘ì—…ë“¤ì„ ìœ„í•œ ì¸ê³µì§€ëŠ¥ ê¸°ì € ëª¨ë¸ë“¤ì˜ í™œìš©},
  author={ìµœì •í˜„ and ë°±í˜¸ì¤€ and ë°•ì°¬ì†” and ê¹€ì¸ì² },
  journal={ë¡œë´‡í•™íšŒ ë…¼ë¬¸ì§€},
  volume={19},
  number={3},
  pages={293--310},
  year={2024}
}
```

---

## Acknowledgement

ì œì•ˆ ëª¨ë¸ì€ [2021 RoboTHOR Challenge](https://github.com/allenai/robothor-challenge) , [CoWs on Pasture](https://github.com/real-stanford/cow), [ESC](https://arxiv.org/pdf/2301.13166)ì˜ ì¼ë¶€ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.

---